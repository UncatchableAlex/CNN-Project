{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00080376],\n",
       "       [-0.00063198]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w = np.array([\n",
    "    [0.1, 0.4],\n",
    "    [0.8, 0.6]\n",
    "])\n",
    "sig = lambda x: 1/(1+np.exp(-x))\n",
    "activation = [sig, lambda x:  x*(1-x)]\n",
    "grad_output = np.array([[-0.00265], [-0.00817]])\n",
    "output = np.array([[0.35], [0.9]])\n",
    "activation[1](output) * (w @ grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.0026569],\n",
       "       [-0.0081723]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.array([\n",
    "    [0.3],\n",
    "    [0.9]\n",
    "])\n",
    "sig = lambda x: 1/(1+np.exp(-x))\n",
    "activation = [sig, lambda x:  x*(1-x)]\n",
    "grad_output = np.array([[-0.0407]])\n",
    "output = np.array([[0.680], [0.664]])\n",
    "activation[1](output) * (w @ grad_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      7\u001b[0m layer1 \u001b[38;5;241m=\u001b[39m Input((\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m, np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.1\u001b[39m,\u001b[38;5;241m0.4\u001b[39m], [\u001b[38;5;241m0.8\u001b[39m,\u001b[38;5;241m0.6\u001b[39m]]))\n\u001b[1;32m----> 8\u001b[0m layer2 \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m layer3 \u001b[38;5;241m=\u001b[39m Output((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m), np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.5\u001b[39m]]), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.35\u001b[39m,\u001b[38;5;241m0.9\u001b[39m]])\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32mc:\\Users\\alexm\\2024_Spring\\final_project\\model\\layers\\dense.py:12\u001b[0m, in \u001b[0;36mDense.__init__\u001b[1;34m(self, input_shape, output_shape, activation, bias)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# input\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape, output_shape, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m):\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# each column represents the weights leading into a node in the next layer\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# each row represents the weights from a node in the last layer\u001b[39;00m\n\u001b[0;32m     15\u001b[0m   \u001b[38;5;66;03m#  self.w = np.random.uniform(-1, 1, (input_shape[0], output_shape[0]))\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull((input_shape[\u001b[38;5;241m0\u001b[39m], output_shape[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;241m1e-4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alexm\\2024_Spring\\final_project\\model\\layers\\layer.py:17\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[1;34m(self, input_shape, output_shape, activation)\u001b[0m\n\u001b[0;32m     11\u001b[0m sig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mnp\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mx))\n\u001b[0;32m     12\u001b[0m activation_funcs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mmaximum(x, \u001b[38;5;241m0.0\u001b[39m), \u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mwhere(x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m)],\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m: [sig, \u001b[38;5;28;01mlambda\u001b[39;00m x:  sig(x)\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39msig(x))],\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     16\u001b[0m }\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m \u001b[43mactivation_funcs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "from model.layers.dense import Dense\n",
    "from model.layers.output import Output\n",
    "from model.layers.input import Input\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "layer1 = Input((2,0), (2,0), 2, np.array([[0.1,0.4], [0.8,0.6]]))\n",
    "layer2 = Dense((2,0), (1,0), 2, np.array([[0.3], [0.9]]))\n",
    "layer3 = Output((1,0), (1,0), np.array([[0.5]]), 'relu', '')\n",
    "\n",
    "input = np.array([[0.35,0.9]]).T\n",
    "sig = lambda x: 1/(1+np.exp(-x))\n",
    "for i in range(100):\n",
    "    final_out = layer3.forward(layer2.forward(layer1.forward(input)))\n",
    "    print(final_out)\n",
    "    layer1.backward(layer2.backward(layer3.backward(final_out)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model that is only fully connected layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69028349]]\n",
      "[[0.68707769]]\n",
      "[[0.68387261]]\n",
      "[[0.68066925]]\n",
      "[[0.67746863]]\n",
      "[[0.67427179]]\n",
      "[[0.67107976]]\n",
      "[[0.66789361]]\n",
      "[[0.66471442]]\n",
      "[[0.66154325]]\n",
      "[[0.6583812]]\n",
      "[[0.65522936]]\n",
      "[[0.65208884]]\n",
      "[[0.64896074]]\n",
      "[[0.64584618]]\n",
      "[[0.64274627]]\n",
      "[[0.63966211]]\n",
      "[[0.63659482]]\n",
      "[[0.63354551]]\n",
      "[[0.63051527]]\n",
      "[[0.6275052]]\n",
      "[[0.62451639]]\n",
      "[[0.62154992]]\n",
      "[[0.61860686]]\n",
      "[[0.61568826]]\n",
      "[[0.61279516]]\n",
      "[[0.60992859]]\n",
      "[[0.60708957]]\n",
      "[[0.60427907]]\n",
      "[[0.60149808]]\n",
      "[[0.59874754]]\n",
      "[[0.59602838]]\n",
      "[[0.59334152]]\n",
      "[[0.59068781]]\n",
      "[[0.58806813]]\n",
      "[[0.58548329]]\n",
      "[[0.58293409]]\n",
      "[[0.58042129]]\n",
      "[[0.57794563]]\n",
      "[[0.57550781]]\n",
      "[[0.57310849]]\n",
      "[[0.57074831]]\n",
      "[[0.56842787]]\n",
      "[[0.56614771]]\n",
      "[[0.56390836]]\n",
      "[[0.56171031]]\n",
      "[[0.55955398]]\n",
      "[[0.5574398]]\n",
      "[[0.5553681]]\n",
      "[[0.55333922]]\n",
      "[[0.55135343]]\n",
      "[[0.54941097]]\n",
      "[[0.54751202]]\n",
      "[[0.54565673]]\n",
      "[[0.54384521]]\n",
      "[[0.54207752]]\n",
      "[[0.54035368]]\n",
      "[[0.53867367]]\n",
      "[[0.53703742]]\n",
      "[[0.53544482]]\n",
      "[[0.53389573]]\n",
      "[[0.53238996]]\n",
      "[[0.53092727]]\n",
      "[[0.5295074]]\n",
      "[[0.52813005]]\n",
      "[[0.52679487]]\n",
      "[[0.52550147]]\n",
      "[[0.52424946]]\n",
      "[[0.52303837]]\n",
      "[[0.52186773]]\n",
      "[[0.52073703]]\n",
      "[[0.51964572]]\n",
      "[[0.51859324]]\n",
      "[[0.51757899]]\n",
      "[[0.51660236]]\n",
      "[[0.51566269]]\n",
      "[[0.51475933]]\n",
      "[[0.51389158]]\n",
      "[[0.51305875]]\n",
      "[[0.51226011]]\n",
      "[[0.51149492]]\n",
      "[[0.51076245]]\n",
      "[[0.51006191]]\n",
      "[[0.50939255]]\n",
      "[[0.50875357]]\n",
      "[[0.5081442]]\n",
      "[[0.50756363]]\n",
      "[[0.50701108]]\n",
      "[[0.50648572]]\n",
      "[[0.50598677]]\n",
      "[[0.50551342]]\n",
      "[[0.50506487]]\n",
      "[[0.50464031]]\n",
      "[[0.50423896]]\n",
      "[[0.50386002]]\n",
      "[[0.5035027]]\n",
      "[[0.50316623]]\n",
      "[[0.50284984]]\n",
      "[[0.50255276]]\n",
      "[[0.50227425]]\n"
     ]
    }
   ],
   "source": [
    "from model.layers.dense import Dense\n",
    "from model.layers.output import Output\n",
    "from model.layers.input import Input\n",
    "from model.optimizers.adam import Adam\n",
    "import numpy as np\n",
    "\n",
    "adam = Adam()\n",
    "layer1 = Input((2,0), (2,0), 2, np.array([[0.1,0.4], [0.8,0.6]]))\n",
    "layer2 = Dense((2,0), (1,0), 2, np.array([[0.3], [0.9]]))\n",
    "layer2.compile(optimizer=adam)\n",
    "layer3 = Output((1,0), (1,0), np.array([[0.5]]), 'relu', '')\n",
    "\n",
    "input = np.array([[0.35,0.9]]).T\n",
    "sig = lambda x: 1/(1+np.exp(-x))\n",
    "for i in range(100):\n",
    "    final_out = layer3.forward(layer2.forward(layer1.forward(input)))\n",
    "    print(final_out)\n",
    "    layer1.backward(layer2.backward(layer3.backward(final_out)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model that has convolutional layers and fully connected layers\n",
    "### but only do backprop on the fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n",
      "[[0.57707703]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model.layers.conv2d import Conv2D\n",
    "from model.layers.output import Output\n",
    "from model.layers.input import Input\n",
    "from model.optimizers.adam import Adam\n",
    "from model.layers.flatten import Flatten\n",
    "from model.layers.dense import Dense\n",
    "import numpy as np\n",
    "\n",
    "adam = Adam()\n",
    "input = np.array([[\n",
    "    [1,2,3,4,5],\n",
    "    [6,7,8,9,10],\n",
    "    [11,12,13,14,15],\n",
    "    [16,17,18,19,20],\n",
    "    [21,22,23,24,25]\n",
    "],\n",
    "[\n",
    "    [30, 31, 32, 33, 34],\n",
    "    [35, 36, 37, 38, 39],\n",
    "    [40, 41, 42, 43, 44],\n",
    "    [45, 46, 47, 48, 49],\n",
    "    [50, 51, 52, 53, 54]]\n",
    "])\n",
    "\n",
    "\n",
    "convLayer = Conv2D(input_shape=(2,5,5), input.shape=(2,3,3), filters=2, kernel_size=3)\n",
    "flattenLayer = Flatten(input_shape=(2,3,3), input.shape=(18,1), activation='relu')\n",
    "flattenLayer.forward(convLayer.forward(input))\n",
    "\n",
    "layer2 = Dense(input_shape=(18,1), input.shape=(5,1))\n",
    "layer3 = Dense(input_shape=(5,1), input.shape=(1,1))\n",
    "\n",
    "layer2.compile(optimizer='adam')\n",
    "layer3.compile(optimizer='adam')\n",
    "output = Output((1,1), (1,1), np.array([[0.50]]), 'relu', '')\n",
    "\n",
    "for i in range(1000):\n",
    "     final_out = output.forward(layer3.forward(layer2.forward(flattenLayer.forward(convLayer.forward(input)))))\n",
    "     print(final_out)\n",
    "     # train fully-connected portion\n",
    "     #layer2.backward(layer3.backward(output.backward(final_out)))\n",
    "     \n",
    "     # train convolutional portion:\n",
    "     \n",
    "\n",
    "#layer3.forward(layer2.forward(flattenLayer.forward(convLayer.forward(input))))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train a network that has convolutional layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50261229]]\n",
      "[[0.5023452]]\n",
      "[[0.50207809]]\n",
      "[[0.5017912]]\n",
      "[[0.50154698]]\n",
      "[[0.50127819]]\n",
      "[[0.5010285]]\n",
      "[[0.50076478]]\n",
      "[[0.50051046]]\n",
      "[[0.50026073]]\n",
      "[[0.50000511]]\n",
      "[[0.49975446]]\n",
      "[[0.49950309]]\n",
      "[[0.4992524]]\n",
      "[[0.49900218]]\n",
      "[[0.49875242]]\n",
      "[[0.4985029]]\n",
      "[[0.49825344]]\n",
      "[[0.49800404]]\n",
      "[[0.4977547]]\n",
      "[[0.49750543]]\n",
      "[[0.49725623]]\n",
      "[[0.4970071]]\n",
      "[[0.49675804]]\n",
      "[[0.49650906]]\n",
      "[[0.49626016]]\n",
      "[[0.49601133]]\n",
      "[[0.49576259]]\n",
      "[[0.49551393]]\n",
      "[[0.49526536]]\n",
      "[[0.49501688]]\n",
      "[[0.49476848]]\n",
      "[[0.49452018]]\n",
      "[[0.49427196]]\n",
      "[[0.49402384]]\n",
      "[[0.49377582]]\n",
      "[[0.49352788]]\n",
      "[[0.49328005]]\n",
      "[[0.49303231]]\n",
      "[[0.49278467]]\n",
      "[[0.49253713]]\n",
      "[[0.49228968]]\n",
      "[[0.49204234]]\n",
      "[[0.4917951]]\n",
      "[[0.49154796]]\n",
      "[[0.49130093]]\n",
      "[[0.49105399]]\n",
      "[[0.49080716]]\n",
      "[[0.49056043]]\n",
      "[[0.49031381]]\n",
      "[[0.49006729]]\n",
      "[[0.48982088]]\n",
      "[[0.48957457]]\n",
      "[[0.48932836]]\n",
      "[[0.48908226]]\n",
      "[[0.48883627]]\n",
      "[[0.48859038]]\n",
      "[[0.4883446]]\n",
      "[[0.48809893]]\n",
      "[[0.48785336]]\n",
      "[[0.48760789]]\n",
      "[[0.48736253]]\n",
      "[[0.48711728]]\n",
      "[[0.48687214]]\n",
      "[[0.4866271]]\n",
      "[[0.48638217]]\n",
      "[[0.48613734]]\n",
      "[[0.48589262]]\n",
      "[[0.48564801]]\n",
      "[[0.4854035]]\n",
      "[[0.4851591]]\n",
      "[[0.4849148]]\n",
      "[[0.48467062]]\n",
      "[[0.48442653]]\n",
      "[[0.48418256]]\n",
      "[[0.48393868]]\n",
      "[[0.48369492]]\n",
      "[[0.48345126]]\n",
      "[[0.48320771]]\n",
      "[[0.48296426]]\n",
      "[[0.48272091]]\n",
      "[[0.48247768]]\n",
      "[[0.48223454]]\n",
      "[[0.48199152]]\n",
      "[[0.48174859]]\n",
      "[[0.48150578]]\n",
      "[[0.48126306]]\n",
      "[[0.48102045]]\n",
      "[[0.48077795]]\n",
      "[[0.48053555]]\n",
      "[[0.48029326]]\n",
      "[[0.48005106]]\n",
      "[[0.47980898]]\n",
      "[[0.47956699]]\n",
      "[[0.47932511]]\n",
      "[[0.47908334]]\n",
      "[[0.47884167]]\n",
      "[[0.4786001]]\n",
      "[[0.47835863]]\n",
      "[[0.47811727]]\n",
      "[[0.47787601]]\n",
      "[[0.47763485]]\n",
      "[[0.4773938]]\n",
      "[[0.47715284]]\n",
      "[[0.47691199]]\n",
      "[[0.47667125]]\n",
      "[[0.4764306]]\n",
      "[[0.47619006]]\n",
      "[[0.47594962]]\n",
      "[[0.47570928]]\n",
      "[[0.47546904]]\n",
      "[[0.4752289]]\n",
      "[[0.47498887]]\n",
      "[[0.47474894]]\n",
      "[[0.4745091]]\n",
      "[[0.47426937]]\n",
      "[[0.47402974]]\n",
      "[[0.47379021]]\n",
      "[[0.47355078]]\n",
      "[[0.47331145]]\n",
      "[[0.47307222]]\n",
      "[[0.47283309]]\n",
      "[[0.47259406]]\n",
      "[[0.47235513]]\n",
      "[[0.4721163]]\n",
      "[[0.47187757]]\n",
      "[[0.47163894]]\n",
      "[[0.47140041]]\n",
      "[[0.47116198]]\n",
      "[[0.47092365]]\n",
      "[[0.47068541]]\n",
      "[[0.47044728]]\n",
      "[[0.47020924]]\n",
      "[[0.4699713]]\n",
      "[[0.46973347]]\n",
      "[[0.46949572]]\n",
      "[[0.46925808]]\n",
      "[[0.46902054]]\n",
      "[[0.46878309]]\n",
      "[[0.46854574]]\n",
      "[[0.46830849]]\n",
      "[[0.46807133]]\n",
      "[[0.46783428]]\n",
      "[[0.46759732]]\n",
      "[[0.46736046]]\n",
      "[[0.46712369]]\n",
      "[[0.46688702]]\n",
      "[[0.46665045]]\n",
      "[[0.46641397]]\n",
      "[[0.4661776]]\n",
      "[[0.46594131]]\n",
      "[[0.46570513]]\n",
      "[[0.46546904]]\n",
      "[[0.46523304]]\n",
      "[[0.46499715]]\n",
      "[[0.46476134]]\n",
      "[[0.46452564]]\n",
      "[[0.46429003]]\n",
      "[[0.46405451]]\n",
      "[[0.46381909]]\n",
      "[[0.46358377]]\n",
      "[[0.46334853]]\n",
      "[[0.4631134]]\n",
      "[[0.46287836]]\n",
      "[[0.46264341]]\n",
      "[[0.46240856]]\n",
      "[[0.46217381]]\n",
      "[[0.46193914]]\n",
      "[[0.46170458]]\n",
      "[[0.4614701]]\n",
      "[[0.46123572]]\n",
      "[[0.46100143]]\n",
      "[[0.46076724]]\n",
      "[[0.46053314]]\n",
      "[[0.46029914]]\n",
      "[[0.46006523]]\n",
      "[[0.45983141]]\n",
      "[[0.45959768]]\n",
      "[[0.45936405]]\n",
      "[[0.45913051]]\n",
      "[[0.45889706]]\n",
      "[[0.45866371]]\n",
      "[[0.45843045]]\n",
      "[[0.45819728]]\n",
      "[[0.4579642]]\n",
      "[[0.45773122]]\n",
      "[[0.45749833]]\n",
      "[[0.45726553]]\n",
      "[[0.45703282]]\n",
      "[[0.45680021]]\n",
      "[[0.45656768]]\n",
      "[[0.45633525]]\n",
      "[[0.45610291]]\n",
      "[[0.45587066]]\n",
      "[[0.4556385]]\n",
      "[[0.45540643]]\n",
      "[[0.45517446]]\n",
      "[[0.45494258]]\n",
      "[[0.45471078]]\n",
      "[[0.45447908]]\n",
      "[[0.45424747]]\n",
      "[[0.45401595]]\n",
      "[[0.45378451]]\n",
      "[[0.45355317]]\n",
      "[[0.45332192]]\n",
      "[[0.45309077]]\n",
      "[[0.4528597]]\n",
      "[[0.45262872]]\n",
      "[[0.45239783]]\n",
      "[[0.45216703]]\n",
      "[[0.45193632]]\n",
      "[[0.4517057]]\n",
      "[[0.45147517]]\n",
      "[[0.45124473]]\n",
      "[[0.45101437]]\n",
      "[[0.45078411]]\n",
      "[[0.45055394]]\n",
      "[[0.45032385]]\n",
      "[[0.45009386]]\n",
      "[[0.44986395]]\n",
      "[[0.44963414]]\n",
      "[[0.44940441]]\n",
      "[[0.44917477]]\n",
      "[[0.44894522]]\n",
      "[[0.44871575]]\n",
      "[[0.44848638]]\n",
      "[[0.44825709]]\n",
      "[[0.44802789]]\n",
      "[[0.44779879]]\n",
      "[[0.44756976]]\n",
      "[[0.44734083]]\n",
      "[[0.44711198]]\n",
      "[[0.44688323]]\n",
      "[[0.44665456]]\n",
      "[[0.44642597]]\n",
      "[[0.44619748]]\n",
      "[[0.44596907]]\n",
      "[[0.44574075]]\n",
      "[[0.44551252]]\n",
      "[[0.44528437]]\n",
      "[[0.44505632]]\n",
      "[[0.44482834]]\n",
      "[[0.44460046]]\n",
      "[[0.44437266]]\n",
      "[[0.44414495]]\n",
      "[[0.44391733]]\n",
      "[[0.44368979]]\n",
      "[[0.44346234]]\n",
      "[[0.44323498]]\n",
      "[[0.4430077]]\n",
      "[[0.44278051]]\n",
      "[[0.44255341]]\n",
      "[[0.44232639]]\n",
      "[[0.44209946]]\n",
      "[[0.44187261]]\n",
      "[[0.44164585]]\n",
      "[[0.44141918]]\n",
      "[[0.44119259]]\n",
      "[[0.44096609]]\n",
      "[[0.44073967]]\n",
      "[[0.44051334]]\n",
      "[[0.4402871]]\n",
      "[[0.44006094]]\n",
      "[[0.43983487]]\n",
      "[[0.43960888]]\n",
      "[[0.43938298]]\n",
      "[[0.43915716]]\n",
      "[[0.43893143]]\n",
      "[[0.43870578]]\n",
      "[[0.43848022]]\n",
      "[[0.43825474]]\n",
      "[[0.43802935]]\n",
      "[[0.43780405]]\n",
      "[[0.43757882]]\n",
      "[[0.43735369]]\n",
      "[[0.43712864]]\n",
      "[[0.43690367]]\n",
      "[[0.43667879]]\n",
      "[[0.43645399]]\n",
      "[[0.43622928]]\n",
      "[[0.43600465]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from model.layers.conv2d import Conv2D\n",
    "from model.layers.output import Output\n",
    "from model.layers.flatten import Flatten\n",
    "from model.layers.dense import Dense\n",
    "from model.layers.maxpool2d import MaxPool2D\n",
    "from model.layers.dropout import ConvDropout, DenseDropout\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "input = np.random.uniform(low=-1, high=1, size=(2,12,12))\n",
    "# make convolutional layers\n",
    "convLayer1 = Conv2D(input_shape=(2,12,12), output_shape=(4,10,10), filters=4, kernel_size=3, activation='relu')\n",
    "convLayer1.compile(optimizer='adam')\n",
    "\n",
    "convDropoutLayer = ConvDropout(input_shape=(4,10,10), output_shape=(4,10,10), dropout_rate=0.5)\n",
    "\n",
    "convLayer2 = Conv2D(input_shape=(4,10,10), output_shape=(10,7,7), filters=10, kernel_size=4, activation='relu')\n",
    "convLayer2.compile(optimizer='adam')\n",
    "\n",
    "poolingLayer = MaxPool2D(input_shape=(10,7,7), output_shape=(10,3,3), pool_size=(5,5))\n",
    "\n",
    "convLayer3 = Conv2D(input_shape=(10,3,3), output_shape=(1,3,3), filters=1, kernel_size=1, activation='relu')\n",
    "convLayer3.compile(optimizer='adam')\n",
    "\n",
    "# make flatten layer\n",
    "flattenLayer = Flatten(input_shape=(1,3,3), output_shape=(9,1), activation='relu')\n",
    "\n",
    "# make fully connected layers\n",
    "layer2 = Dense(input_shape=(9,1), output_shape=(5,1), activation='relu')\n",
    "layer2.compile(optimizer='adam')\n",
    "\n",
    "denseDropoutLayer = DenseDropout(input_shape=(5,1), output_shape=(5,1), dropout_rate=0.5)\n",
    "\n",
    "layer3 = Dense(input_shape=(5,1), output_shape=(1,1), activation='relu')\n",
    "layer3.compile(optimizer='adam')\n",
    "\n",
    "# make output\n",
    "output = Output((1,1), (1,1), np.array([[0.24]]), 'sigmoid', '')\n",
    "\n",
    "iters = 2000\n",
    "for i in range(iters):\n",
    "     conv_out = convLayer3.forward(poolingLayer.forward(convLayer2.forward(convDropoutLayer.forward(convLayer1.forward(input)))))\n",
    "     final_out = output.forward(layer3.forward(denseDropoutLayer.forward(layer2.forward(flattenLayer.forward(conv_out)))))\n",
    "     print(final_out)\n",
    "     # train fully-connected portion\n",
    "     fully_connected_backprop = layer2.backward(denseDropoutLayer.backward(layer3.backward(output.backward(final_out))))\n",
    "     # train convolutional portion:\n",
    "     convLayer1.backward(convDropoutLayer.backward(convLayer2.backward(poolingLayer.backward(convLayer3.backward(flattenLayer.backward(fully_connected_backprop))))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple channel example of maxpooling backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([[\n\u001b[0;32m      2\u001b[0m     [\u001b[38;5;241m111\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m],\n\u001b[0;32m      3\u001b[0m     [\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m10\u001b[39m],\n\u001b[0;32m      4\u001b[0m     [\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m13\u001b[39m,\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m15\u001b[39m],\n\u001b[0;32m      5\u001b[0m     [\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m17\u001b[39m,\u001b[38;5;241m18\u001b[39m,\u001b[38;5;241m19\u001b[39m,\u001b[38;5;241m20\u001b[39m],\n\u001b[0;32m      6\u001b[0m     [\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m22\u001b[39m,\u001b[38;5;241m23\u001b[39m,\u001b[38;5;241m24\u001b[39m,\u001b[38;5;241m25\u001b[39m]\n\u001b[0;32m      7\u001b[0m ],\n\u001b[0;32m      8\u001b[0m [\n\u001b[0;32m      9\u001b[0m     [\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m31\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m33\u001b[39m, \u001b[38;5;241m34\u001b[39m],\n\u001b[0;32m     10\u001b[0m     [\u001b[38;5;241m35\u001b[39m, \u001b[38;5;241m36\u001b[39m, \u001b[38;5;241m37\u001b[39m, \u001b[38;5;241m38\u001b[39m, \u001b[38;5;241m39\u001b[39m],\n\u001b[0;32m     11\u001b[0m     [\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m41\u001b[39m, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m43\u001b[39m, \u001b[38;5;241m44\u001b[39m],\n\u001b[0;32m     12\u001b[0m     [\u001b[38;5;241m45\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m44\u001b[39m, \u001b[38;5;241m488\u001b[39m, \u001b[38;5;241m49\u001b[39m],\n\u001b[0;32m     13\u001b[0m     [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m51\u001b[39m, \u001b[38;5;241m52\u001b[39m, \u001b[38;5;241m53\u001b[39m, \u001b[38;5;241m54\u001b[39m]]\n\u001b[0;32m     14\u001b[0m ])\n\u001b[0;32m     16\u001b[0m pool_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     17\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "input = np.array([[\n",
    "    [111,2,3,4,5],\n",
    "    [6,7,8,9,10],\n",
    "    [11,12,13,14,15],\n",
    "    [16,17,18,19,20],\n",
    "    [21,22,23,24,25]\n",
    "],\n",
    "[\n",
    "    [30, 31, 32, 33, 34],\n",
    "    [35, 36, 37, 38, 39],\n",
    "    [40, 41, 42, 43, 44],\n",
    "    [45, 4, 44, 488, 49],\n",
    "    [50, 51, 52, 53, 54]]\n",
    "])\n",
    "\n",
    "pool_size = (3,3)\n",
    "stride = 1\n",
    "output_size = int((input.shape[1] - pool_size[0])/stride) + 1\n",
    "output = np.zeros((input.shape[0], output_size, output_size))\n",
    "positions = np.zeros(output.shape, dtype=(int, 3))\n",
    "for i in range(output_size): # rows\n",
    "    for j in range(output_size): # columns\n",
    "        row_start = i*stride\n",
    "        row_end = row_start + pool_size[0]\n",
    "        col_start = j*stride\n",
    "        col_end = col_start + pool_size[1]\n",
    "        for k in range(input.shape[0]): # channels\n",
    "            frame = input[k,row_start:row_end, col_start:col_end]\n",
    "            output[k,i,j] = np.max(frame)\n",
    "            # get the spot in the frame with the max value\n",
    "            positions[k,i,j,1:] = np.unravel_index(np.argmax(frame), frame.shape)\n",
    "            # add channel to position\n",
    "            positions[k,i,j,0] = k\n",
    "            # add offsets to get the spot in the output matrix with max value of this frame\n",
    "            positions[k,i,j,1] += row_start\n",
    "            positions[k,i,j,2] += col_start\n",
    "            \n",
    "output_grads = np.array(\n",
    "    [\n",
    "        [\n",
    "            [1, 2, 3],\n",
    "            [4,5,6],\n",
    "            [7,8,9]\n",
    "        ],\n",
    "\n",
    "        [\n",
    "            [10,11,12],\n",
    "            [13,14,15],\n",
    "            [16,17,18]\n",
    "        ],\n",
    "    ]\n",
    ")\n",
    "r_val = np.zeros(input.shape)\n",
    "channel_pos = positions[:, :, :, 0]\n",
    "row_pos = positions[:, :, :, 1]\n",
    "col_pos = positions[:, :, :, 2]\n",
    "r_val[channel_pos, row_pos, col_pos] = output_grads\n",
    "print(input)\n",
    "print(output)\n",
    "print(r_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple channel example of dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 1]\n",
      " [1 1 0 3 1 4 1 1 0 0 4 4 0 2 1 4 0 0 0 1 2 1 0 0 4]\n",
      " [2 3 1 0 0 4 0 2 0 0 3 2 1 3 0 3 3 0 1 0 4 4 3 4 1]]\n",
      "[[[  0   0   3   0   0]\n",
      "  [  0   7   0   0  10]\n",
      "  [ 11  12  13   0  15]\n",
      "  [ 16  17  18  19  20]\n",
      "  [ 21  22  23  24   0]]\n",
      "\n",
      " [[  0   0  32   0  34]\n",
      "  [  0  36   0  38   0]\n",
      "  [ 40  41  42  43   0]\n",
      "  [  0   4  44 488  49]\n",
      "  [ 50   0   0   0  54]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[  0,   0,   3,   0,   0],\n",
       "        [  0,   7,   0,   0,  10],\n",
       "        [ 11,  12,  13,   0,  15],\n",
       "        [ 16,  17,  18,  19,  20],\n",
       "        [ 21,  22,  23,  24,   0]],\n",
       "\n",
       "       [[  0,   0,  32,   0,  34],\n",
       "        [  0,  36,   0,  38,   0],\n",
       "        [ 40,  41,  42,  43,   0],\n",
       "        [  0,   4,  44, 488,  49],\n",
       "        [ 50,   0,   0,   0,  54]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = np.array([[\n",
    "    [111,2,3,4,5],\n",
    "    [6,7,8,9,10],\n",
    "    [11,12,13,14,15],\n",
    "    [16,17,18,19,20],\n",
    "    [21,22,23,24,25]\n",
    "],\n",
    "[\n",
    "    [30, 31, 32, 33, 34],\n",
    "    [35, 36, 37, 38, 39],\n",
    "    [40, 41, 42, 43, 44],\n",
    "    [45, 4, 44, 488, 49],\n",
    "    [50, 51, 52, 53, 54]]\n",
    "])\n",
    "input_shape = input.shape\n",
    "dropout_rate = 0.5\n",
    "\n",
    "dropout_num = int(input_shape[0] * input_shape[1] * input_shape[2] * dropout_rate)\n",
    "z_coord = np.random.randint(low=0, high=input.shape[0], size=dropout_num)\n",
    "x_coord = np.random.randint(low=0, high=input_shape[1], size=dropout_num)\n",
    "y_coord = np.random.randint(low=0, high=input_shape[2], size=dropout_num)\n",
    "positions = np.stack((z_coord, x_coord, y_coord))\n",
    "print(positions)\n",
    "input_copy = np.copy(input)\n",
    "input_copy[z_coord,x_coord,y_coord] = 0.0\n",
    "print(input_copy)\n",
    "\n",
    "\n",
    "output_grads = input\n",
    "# this is a matrix of the channel coordinates for each of the max values that we outputed upon forward prop\n",
    "channel_pos = positions[0]\n",
    "# and a matrix for the row coordinates\n",
    "row_pos = positions[1]\n",
    "# and a matrix for the col coordinates\n",
    "col_pos = positions[2]\n",
    "output_grads[channel_pos, row_pos, col_pos] = 0.0\n",
    "output_grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.50264213]]\n",
      "[[0.50236389]]\n",
      "[[0.50208917]]\n",
      "[[0.50181788]]\n",
      "[[0.50154991]]\n",
      "[[0.50128512]]\n",
      "[[0.50102334]]\n",
      "[[0.50076437]]\n",
      "[[0.50050794]]\n",
      "[[0.50025379]]\n",
      "[[0.50000161]]\n",
      "[[0.49975104]]\n",
      "[[0.49950131]]\n",
      "[[0.49925164]]\n",
      "[[0.49900202]]\n",
      "[[0.49875244]]\n",
      "[[0.49850293]]\n",
      "[[0.49825347]]\n",
      "[[0.49800407]]\n",
      "[[0.49775474]]\n",
      "[[0.49750547]]\n",
      "[[0.49725627]]\n",
      "[[0.49700714]]\n",
      "[[0.49675808]]\n",
      "[[0.49650911]]\n",
      "[[0.4962602]]\n",
      "[[0.49601138]]\n",
      "[[0.49576264]]\n",
      "[[0.49551399]]\n",
      "[[0.49526542]]\n",
      "[[0.49501694]]\n",
      "[[0.49476854]]\n",
      "[[0.49452024]]\n",
      "[[0.49427203]]\n",
      "[[0.49402391]]\n",
      "[[0.49377588]]\n",
      "[[0.49352795]]\n",
      "[[0.49328012]]\n",
      "[[0.49303238]]\n",
      "[[0.49278474]]\n",
      "[[0.4925372]]\n",
      "[[0.49228976]]\n",
      "[[0.49204242]]\n",
      "[[0.49179518]]\n",
      "[[0.49154804]]\n",
      "[[0.49130101]]\n",
      "[[0.49105407]]\n",
      "[[0.49080724]]\n",
      "[[0.49056052]]\n",
      "[[0.49031389]]\n",
      "[[0.49006738]]\n",
      "[[0.48982096]]\n",
      "[[0.48957465]]\n",
      "[[0.48932845]]\n",
      "[[0.48908235]]\n",
      "[[0.48883636]]\n",
      "[[0.48859047]]\n",
      "[[0.48834469]]\n",
      "[[0.48809902]]\n",
      "[[0.48785345]]\n",
      "[[0.48760799]]\n",
      "[[0.48736263]]\n",
      "[[0.48711738]]\n",
      "[[0.48687223]]\n",
      "[[0.4866272]]\n",
      "[[0.48638226]]\n",
      "[[0.48613744]]\n",
      "[[0.48589272]]\n",
      "[[0.48564811]]\n",
      "[[0.4854036]]\n",
      "[[0.4851592]]\n",
      "[[0.48491491]]\n",
      "[[0.48467072]]\n",
      "[[0.48442664]]\n",
      "[[0.48418266]]\n",
      "[[0.48393879]]\n",
      "[[0.48369502]]\n",
      "[[0.48345136]]\n",
      "[[0.48320781]]\n",
      "[[0.48296436]]\n",
      "[[0.48272102]]\n",
      "[[0.48247778]]\n",
      "[[0.48223465]]\n",
      "[[0.48199162]]\n",
      "[[0.4817487]]\n",
      "[[0.48150589]]\n",
      "[[0.48126317]]\n",
      "[[0.48102056]]\n",
      "[[0.48077806]]\n",
      "[[0.48053566]]\n",
      "[[0.48029337]]\n",
      "[[0.48005118]]\n",
      "[[0.47980909]]\n",
      "[[0.47956711]]\n",
      "[[0.47932523]]\n",
      "[[0.47908345]]\n",
      "[[0.47884178]]\n",
      "[[0.47860021]]\n",
      "[[0.47835875]]\n",
      "[[0.47811738]]\n",
      "[[0.47787612]]\n",
      "[[0.47763497]]\n",
      "[[0.47739391]]\n",
      "[[0.47715296]]\n",
      "[[0.47691211]]\n",
      "[[0.47667137]]\n",
      "[[0.47643072]]\n",
      "[[0.47619018]]\n",
      "[[0.47594974]]\n",
      "[[0.4757094]]\n",
      "[[0.47546916]]\n",
      "[[0.47522902]]\n",
      "[[0.47498899]]\n",
      "[[0.47474906]]\n",
      "[[0.47450922]]\n",
      "[[0.47426949]]\n",
      "[[0.47402986]]\n",
      "[[0.47379033]]\n",
      "[[0.4735509]]\n",
      "[[0.47331157]]\n",
      "[[0.47307234]]\n",
      "[[0.47283321]]\n",
      "[[0.47259419]]\n",
      "[[0.47235526]]\n",
      "[[0.47211643]]\n",
      "[[0.4718777]]\n",
      "[[0.47163907]]\n",
      "[[0.47140054]]\n",
      "[[0.47116211]]\n",
      "[[0.47092377]]\n",
      "[[0.47068554]]\n",
      "[[0.4704474]]\n",
      "[[0.47020937]]\n",
      "[[0.46997143]]\n",
      "[[0.46973359]]\n",
      "[[0.46949585]]\n",
      "[[0.46925821]]\n",
      "[[0.46902066]]\n",
      "[[0.46878322]]\n",
      "[[0.46854587]]\n",
      "[[0.46830862]]\n",
      "[[0.46807146]]\n",
      "[[0.4678344]]\n",
      "[[0.46759745]]\n",
      "[[0.46736058]]\n",
      "[[0.46712382]]\n",
      "[[0.46688715]]\n",
      "[[0.46665058]]\n",
      "[[0.4664141]]\n",
      "[[0.46617772]]\n"
     ]
    }
   ],
   "source": [
    "from model.layers.conv2d import Conv2D\n",
    "from model.layers.output import Output\n",
    "from model.layers.flatten import Flatten\n",
    "from model.layers.dense import Dense\n",
    "from model.layers.maxpool2d import MaxPool2D\n",
    "from model.layers.dropout import ConvDropout, DenseDropout\n",
    "import numpy as np\n",
    "from sequenal import Sequential\n",
    "\n",
    "\n",
    "from model.layers.conv2d import Conv2D\n",
    "from model.layers.output import Output\n",
    "from model.layers.flatten import Flatten\n",
    "from model.layers.dense import Dense\n",
    "from model.layers.maxpool2d import MaxPool2D\n",
    "from model.layers.dropout import ConvDropout, DenseDropout\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "input = np.random.uniform(low=-1, high=1, size=(2,12,12))\n",
    "# make convolutional layers\n",
    "convLayer1 = Conv2D(input_shape=(2,12,12), output_shape=(4,10,10), filters=4, kernel_size=3, activation='relu')\n",
    "convDropoutLayer = ConvDropout(input_shape=(4,10,10), output_shape=(4,10,10), dropout_rate=0.5)\n",
    "convLayer2 = Conv2D(input_shape=(4,10,10), output_shape=(10,7,7), filters=10, kernel_size=4, activation='relu')\n",
    "poolingLayer = MaxPool2D(input_shape=(10,7,7), output_shape=(10,3,3), pool_size=(5,5))\n",
    "convLayer3 = Conv2D(input_shape=(10,3,3), output_shape=(1,3,3), filters=1, kernel_size=1, activation='relu')\n",
    "\n",
    "# make flatten layer\n",
    "flattenLayer = Flatten(input_shape=(1,3,3), output_shape=(9,1), activation='relu')\n",
    "layer2 = Dense(input_shape=(9,1), output_shape=(5,1), activation='relu')\n",
    "denseDropoutLayer = DenseDropout(input_shape=(5,1), output_shape=(5,1), dropout_rate=0.5)\n",
    "layer3 = Dense(input_shape=(5,1), output_shape=(1,1), activation='relu')\n",
    "# make output\n",
    "output = Output((1,1), (1,1), np.array([[0.24]]), 'sigmoid', '')\n",
    "input = np.random.uniform(low=-1, high=1, size=(2,12,12))\n",
    "sequential = Sequential()\n",
    "\n",
    "\n",
    "sequential.add(convLayer1)\n",
    "sequential.add(convLayer2)\n",
    "sequential.add(poolingLayer)\n",
    "sequential.add(convLayer3)\n",
    "sequential.add(flattenLayer)\n",
    "sequential.add(layer2)\n",
    "sequential.add(layer3)\n",
    "sequential.add(output)\n",
    "\n",
    "sequential.compile('adam')\n",
    "\n",
    "\n",
    "for i in range(150):\n",
    "    final_out =sequential.forward(input)\n",
    "    print(final_out)\n",
    "    sequential.backward(final_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
